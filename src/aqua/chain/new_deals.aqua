module NewDeals declares poll_new_deals

import or, not from "@fluencelabs/aqua-lib/binary.aqua"
import Spell, TriggerConfig from "@fluencelabs/spell/spell_service.aqua"

import FluenceAuroraConnector, DealCreated from "services.aqua"

import AuroraInfo from "aurora.aqua"
import deal_log, spell_log, get_counter, get_string from "../fluence/spell.aqua"
import is_worker_created from "../fluence/worker.aqua"
import Json from "../fluence/peer.aqua"
import join_deal from "../decider/join_deal.aqua"
import SpellId from "../types.aqua"

use "last_seen.aqua" as LastSeen

-- return a value to use as a left boundary in `eth_newLogs`
-- it usually equals to `last seen block`, except on first iteration it is equal to evaluated `from_block`
func get_left_boundary(spell_id: SpellId, chain: AuroraInfo) -> ?string:
    left: ?string

    -- load "last seen" block number
    -- `last seen block` is initialized to the first `left boundary` this function returns
    -- After that, `last seen block` is set to the block number of every next processed Log
    maybe_last_seen <- LastSeen.get(spell_id)
    if maybe_last_seen == nil:
        -- When have not yet seen any blocks, initialize `last seen` to `init_data.from_block`
        last_seen <- LastSeen.init(spell_id, chain)
        if last_seen != nil:
            left <<- last_seen!
        -- if `LastSeen.init` returned `nil`, just bubble up that `nil` by returning it
    else:
        left <<- maybe_last_seen!
    <- left

func join_deals(spell_id: SpellId, logs: []DealCreated, left: string):
    Spell spell_id

    -- TODO: I assume that `logs` are sorted by `block_number`. Is that a correct assumption?
    for log <- logs:
        deal_id = log.info.deal_id
        if is_worker_created(spell_id, deal_id):
            deal_log(spell_id, deal_id, "worker for deal is already created")
        else:
            join_deal(spell_id, log)

        -- Use `log.block_number - 1` as a `last seen block`
        -- Can not use `log.block_number` because it is unknown if there are more events from that block in `logs`
        -- If a log from block N is received, then the whole `N-1` block was processed, so save it as the `last seen`
        prev_block <- FluenceAuroraConnector.hex_sub(log.block_number, 1)
        if prev_block.success:
            -- Set `last seen block` to the `log.block_number - 1`
            -- This way, if particle has timed out during process of a log, `last seen block` would not be updated
            -- And we will try all events from that block again
            LastSeen.increase(spell_id, prev_block.diff!, left)
        else:
            deal_log(spell_id, deal_id, ["error in hex_sub", prev_block])

    -- after we have processed all logs, we can conclude
    -- that we have seen all logs in mentioned blocks
    LastSeen.save(spell_id, logs[logs.length - 1].block_number)

func poll_logs(spell_id: SpellId, chain: AuroraInfo, left: string) -> []DealCreated:
    result <- FluenceAuroraConnector.poll_deal_created(chain.api_endpoint, chain.address, left)
    if result.success == false:
        spell_log(spell_id, ["error polling deal created events", result.error])

    logs = result.result
    spell_log(spell_id, ["number of created deals from poll:", logs.length, "from block:", left])

    <- logs

func poll_new_deals(spell_id: SpellId, chain: AuroraInfo):
    -- retrieve block number to poll from
    left <- get_left_boundary(spell_id, chain)
    if left != nil:
        logs <- poll_logs(spell_id, chain, left!)
        join_deals(spell_id, logs, left!)
    else:
        -- if it's not possible to retrieve `left boundary`, then it's not possible to `poll_logs`
        spell_log(spell_id, ["get_left_boundary returns nil, unable to proceed"])